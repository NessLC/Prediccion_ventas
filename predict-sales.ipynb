{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Resumen\n\nEste proyecto se desarrollo totalmente en los núcleos de kaggle. Se entrenan varios modelos y cada uno demanda varias horas de entrenamiento.<br>\nPara poder hacer cambios y pruebas se implementó un sistema que activa o desactiva los entrenamientos y evaluaciones de cada modelo.<br>\nXGBoost es el que más tiempo toma quedando en el limite de las 9h.<br><br>\nCada entrenamiento se hace independiente y posteriormente se juntan todos mendiante un staking\nLos resultados se acumulan en un Dataset de kaggle https://www.kaggle.com/nesslc/sales-cv-for-s para leer desde ahi los archivos necesarios y finalmente entrenar un metamodel para hacer la predicción final.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVR\nfrom sklearn import model_selection\n\n\nimport math\n\nfrom itertools import product\nimport ipywidgets as widgets\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom tqdm import tqdm\n\nimport gc\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-15T03:03:34.400282Z","iopub.execute_input":"2021-10-15T03:03:34.400686Z","iopub.status.idle":"2021-10-15T03:03:37.813522Z","shell.execute_reply.started":"2021-10-15T03:03:34.400591Z","shell.execute_reply":"2021-10-15T03:03:37.812825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Este es el vector que activa las partes en el siguiente orden:\n\n\n#LastMonth,Baseline1,Baseline2,    XGBoost, LigthGB, SVM, MLP,    Reed from DS, Meta-model\n\npartes_activas=[False,False,False,   False,False,False,False,        True,True]","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:04:55.676499Z","iopub.execute_input":"2021-10-15T03:04:55.677434Z","iopub.status.idle":"2021-10-15T03:04:55.682056Z","shell.execute_reply.started":"2021-10-15T03:04:55.67739Z","shell.execute_reply":"2021-10-15T03:04:55.681102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cargar datos","metadata":{}},{"cell_type":"code","source":"sales_train = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\nitem_categories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\nsample_submission = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:04:57.901566Z","iopub.execute_input":"2021-10-15T03:04:57.902356Z","iopub.status.idle":"2021-10-15T03:05:00.756605Z","shell.execute_reply.started":"2021-10-15T03:04:57.902314Z","shell.execute_reply":"2021-10-15T03:05:00.755485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_names = ['sales_train', 'shops','items', 'item_categories','test']\n\nkaggle_data_tabs = widgets.Tab()\n# Add Output widgets for each pandas DF as tabs' children\nkaggle_data_tabs.children = list([widgets.Output() for df_name in df_names])\n\nfor index in range(0, len(df_names)):\n    # Rename tab bar titles to df names\n    kaggle_data_tabs.set_title(index, df_names[index])\n    \n    # Display corresponding table output for this tab name\n    with kaggle_data_tabs.children[index]:\n        display(eval(df_names[index]))\n\ndisplay(kaggle_data_tabs)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:05:00.758759Z","iopub.execute_input":"2021-10-15T03:05:00.759117Z","iopub.status.idle":"2021-10-15T03:05:00.892102Z","shell.execute_reply.started":"2021-10-15T03:05:00.759075Z","shell.execute_reply":"2021-10-15T03:05:00.89097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(sales_train['shop_id'].unique()),len(sales_train['item_id'].unique()))\nprint(len(sample_submission))","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:05:00.893472Z","iopub.execute_input":"2021-10-15T03:05:00.894996Z","iopub.status.idle":"2021-10-15T03:05:00.95858Z","shell.execute_reply.started":"2021-10-15T03:05:00.894948Z","shell.execute_reply":"2021-10-15T03:05:00.957779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lo más básico de lo básico\nLa primera idea es ver que tan bien sale el score si usamos los datos del mes anterior y pasarlos tal cual como los datos de prediccion. ","metadata":{}},{"cell_type":"code","source":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\nif partes_activas[0]:\n    #Obtener las ventas totales de cada tienda-articulo \n    last = sales_train[sales_train['date_block_num']==33]\n    grupos = last.groupby(['shop_id','item_id','date_block_num'])\n\n    \n\n    base0 = grupos.agg({'item_cnt_day':'sum', 'item_price':'mean'}).reset_index()\n    base0 = base0.rename(columns = {'item_cnt_day' : 'item_cnt_month'})\n    base0['item_cnt_month'] = base0['item_cnt_month'].clip(0,20)#Esto lo sugiere el organizador\n    base0","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:05:02.006565Z","iopub.execute_input":"2021-10-15T03:05:02.006847Z","iopub.status.idle":"2021-10-15T03:05:02.013638Z","shell.execute_reply.started":"2021-10-15T03:05:02.00682Z","shell.execute_reply":"2021-10-15T03:05:02.012789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unir los valores obtenidos en el orden solicitado de ID para la submission final\nif partes_activas[0]:\n    submission_base0 = test.merge(base0,how='left',on=['shop_id','item_id']).fillna(0)\n    submission_base0 = submission_base0.drop(columns=index_cols+['item_price'])\n    submission_base0.to_csv('submision_base0.csv',index=False)\n    submission_base0","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:05:02.799367Z","iopub.execute_input":"2021-10-15T03:05:02.799932Z","iopub.status.idle":"2021-10-15T03:05:02.805444Z","shell.execute_reply.started":"2021-10-15T03:05:02.799893Z","shell.execute_reply":"2021-10-15T03:05:02.804689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Un Modelo Basico\nAhora se entrenará un modelo base utilizando un modelo XGB  <br/>\nPrimero tengo que formar un set de entrenamiento","metadata":{}},{"cell_type":"code","source":"#Se puede hacer una malla de cada mes con los pares de tienda-articulo\ngrid = []\nfor mes in sales_train['date_block_num'].unique():\n    shops_mes = sales_train[sales_train['date_block_num']==mes]['shop_id'].unique()\n    items_mes = sales_train[sales_train['date_block_num']==mes]['item_id'].unique()\n    grid.append(np.array(list(product(*[shops_mes, items_mes, [mes]])),dtype='int32'))\n\n\nfull_grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# ahora unimos la información que tenemos\n\ngrupos = sales_train.groupby(index_cols)\n\nsales_agg = grupos.agg({'item_cnt_day':'sum', 'item_price':'mean'}).reset_index()\nsales_agg = sales_agg.rename(columns = {'item_cnt_day' : 'item_cnt_month'})\nsales_agg['item_cnt_month'] = sales_agg['item_cnt_month'].clip(0,20)#Esto lo sugiere el organizador\n\ntrain_set = full_grid.merge(sales_agg,how='left',on=index_cols).fillna(0)\n# unimos la información de categoria de item\ntrain_set = train_set.merge(items.drop(columns=['item_name']),how='left',on=['item_id']).astype({\"item_cnt_month\": int})\ntrain_set","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:05:04.772772Z","iopub.execute_input":"2021-10-15T03:05:04.77304Z","iopub.status.idle":"2021-10-15T03:05:35.646554Z","shell.execute_reply.started":"2021-10-15T03:05:04.773013Z","shell.execute_reply":"2021-10-15T03:05:35.645775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['shop_id','item_id','date_block_num','item_category_id']\nX_train = train_set[features]#.set_index('shop_id')\ny_train = train_set['item_cnt_month']\nX_train","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:05:35.647992Z","iopub.execute_input":"2021-10-15T03:05:35.648222Z","iopub.status.idle":"2021-10-15T03:05:35.976599Z","shell.execute_reply.started":"2021-10-15T03:05:35.648196Z","shell.execute_reply":"2021-10-15T03:05:35.975831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make test_dataset pandas data frame, add category id and date block num, then convert back to numpy array and predict\nmerged_test = pd.merge(test, items,how='left', on = ['item_id'])[['shop_id','item_id','item_category_id']]\nmerged_test['date_block_num'] = 34\n#merged_test = merged_test.set_index('shop_id')\nmerged_test","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:05:35.977994Z","iopub.execute_input":"2021-10-15T03:05:35.978736Z","iopub.status.idle":"2021-10-15T03:05:36.030644Z","shell.execute_reply.started":"2021-10-15T03:05:35.978677Z","shell.execute_reply":"2021-10-15T03:05:36.029833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if partes_activas[1]:\n    model_base = xgb.XGBRegressor(max_depth = 10, min_child_weight=0.5, subsample = 1, eta = 0.3, n_estimators = 1000, seed = 1)\n    model_base.fit(X_train, y_train, eval_metric='rmse')\n\n    preds = model_base.predict(merged_test.values)\n\n    preds_df = pd.DataFrame(preds,columns=['item_cnt_month'])\n    preds_df['ID'] = preds_df.index\n    preds_df = preds_df.set_index('ID')\n    preds_df.to_csv('submision_base.csv')\n    display(preds_df)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:05:36.033115Z","iopub.execute_input":"2021-10-15T03:05:36.03358Z","iopub.status.idle":"2021-10-15T03:05:36.040366Z","shell.execute_reply.started":"2021-10-15T03:05:36.033539Z","shell.execute_reply":"2021-10-15T03:05:36.039829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Esta versión basica da al rededor de 3.3 en el score lo cual es muuuy malo","metadata":{}},{"cell_type":"markdown","source":"# un poco de feature engenering\nintentaremos hacer un poco de feature engenering y ajustar modelos mas trabajados. Las dos primeras cosas que se ocurren es agregar las ventas del mes o meses anteriores por cada tienda-articulo y la otra las ventas de ese articulo el mismo mes del año anterior.","metadata":{}},{"cell_type":"code","source":"# todas las features que agreguemos se tienen que hacer en train y test\n\ntrain_test = pd.concat([train_set, merged_test], axis = 0)\n\n\n\n# Agregarems las ventas anteriores de cada mes durante todo un año para cada tienda-item \nmeses_anteriores  = [1,2,3,12] \n\nfor mes in tqdm(meses_anteriores):\n    train_test_aux = train_test.copy()\n    nombre_feature = 'prev_shopitem_sales_' + str(mes)\n    train_test_aux.loc[:,'date_block_num'] += mes\n    \n    train_test_aux.rename(columns={'item_cnt_month': nombre_feature}, inplace=True)\n    \n    \n    train_test = train_test.merge(train_test_aux[['shop_id', 'item_id', 'date_block_num', nombre_feature]], \n                                  on = ['shop_id', 'item_id', 'date_block_num'], how = 'left').fillna(0)\n\n    \n\n    \ngroups = train_test.groupby(by = ['item_id', 'date_block_num'])\nfor mes in tqdm(meses_anteriores):\n    nombre_feature = 'prev_item_sales_' + str(mes)\n    result = groups.agg({'item_cnt_month':'mean'})\n    result = result.reset_index()\n    result.loc[:, 'date_block_num'] += mes\n    result.rename(columns={'item_cnt_month': nombre_feature}, inplace=True)\n    train_test = train_test.merge(result, on = ['item_id', 'date_block_num'], how = 'left')\n    train_test[nombre_feature] = train_test[nombre_feature].fillna(0)\n    \n    \n\n\n    \ndef past_information(df, merging_cols, new_col, aggregation):\n    temp = []\n    for i in tqdm(range(1,35)):\n        block = df.query(f'date_block_num < {i}').groupby(merging_cols).agg(aggregation).reset_index()\n        block.columns = merging_cols + [new_col]\n        block['date_block_num'] = i\n        block = block[block[new_col]>0]\n        temp.append(block)\n    temp = pd.concat(temp)\n    df = pd.merge(df, temp, on=['date_block_num']+merging_cols, how='left')\n    return df\n\n#average item price in latest block item was sold\ntrain_test = past_information(train_test, ['item_id'],'last_item_price',{'item_price':'last'})\n           \n#total units of item sold at individual shop\ntrain_test = past_information(train_test, ['shop_id','item_id'],'item_cnt_sum_alltime',{'item_cnt_month':'sum'})\n\n#total units of item sold at all shops\ntrain_test = past_information(train_test, ['item_id'],'item_cnt_sum_alltime_allshops',{'item_cnt_month':'sum'})\n\ntrain_test = past_information(train_test,)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:05:36.041931Z","iopub.execute_input":"2021-10-15T03:05:36.042374Z","iopub.status.idle":"2021-10-15T03:08:13.81575Z","shell.execute_reply.started":"2021-10-15T03:05:36.042335Z","shell.execute_reply":"2021-10-15T03:08:13.814648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mean encoding","metadata":{}},{"cell_type":"code","source":"def mean_encode(train_test,to_encode):\n\n    \n    kf = model_selection.KFold(5, shuffle=False)\n    feature_name = to_encode+'_enc'\n    train_test[feature_name] = np.nan\n    train_test[feature_name].fillna(0.3343, inplace=True)\n    #all_data = train_test[train_test['date_block_num']<34]\n    for tr_ind, val_ind in kf.split(train_test):\n        X_tr, X_val = train_test.iloc[tr_ind], train_test.iloc[val_ind]\n        train_test.loc[train_test.index[val_ind], feature_name] = X_val[to_encode].map(X_tr.groupby(to_encode).item_cnt_month.mean())\n        #train_test[feature_name].fillna(0.3343, inplace=True)\n    return train_test\n    \n    \n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:08:13.81784Z","iopub.execute_input":"2021-10-15T03:08:13.818101Z","iopub.status.idle":"2021-10-15T03:08:13.825141Z","shell.execute_reply.started":"2021-10-15T03:08:13.818073Z","shell.execute_reply":"2021-10-15T03:08:13.824489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_to_encode =['shop_id','item_id','item_category_id']\n\nfor element in tqdm(features_to_encode):\n    train_test = mean_encode(train_test,element)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:08:13.826332Z","iopub.execute_input":"2021-10-15T03:08:13.826794Z","iopub.status.idle":"2021-10-15T03:08:39.957067Z","shell.execute_reply.started":"2021-10-15T03:08:13.826758Z","shell.execute_reply":"2021-10-15T03:08:39.956084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test =train_test.fillna(0)\ntrain_test","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:08:39.958164Z","iopub.execute_input":"2021-10-15T03:08:39.95837Z","iopub.status.idle":"2021-10-15T03:08:41.069635Z","shell.execute_reply.started":"2021-10-15T03:08:39.958344Z","shell.execute_reply":"2021-10-15T03:08:41.068826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if partes_activas[2]:\n    msk = train_test['date_block_num']==34\n    X_train = train_test[~msk]\n    y_train = X_train['item_cnt_month']\n    X_train = X_train.drop(columns=['item_cnt_month','item_price'])\n    \n    X_test = train_test[msk].drop(columns=['item_cnt_month','item_price'])","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:08:41.070928Z","iopub.execute_input":"2021-10-15T03:08:41.071163Z","iopub.status.idle":"2021-10-15T03:08:41.076748Z","shell.execute_reply.started":"2021-10-15T03:08:41.071136Z","shell.execute_reply":"2021-10-15T03:08:41.075752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if partes_activas[2]:\n    model_base = xgb.XGBRegressor(max_depth = 10, min_child_weight=0.5, subsample = 1, eta = 0.3, n_estimators = 1000, seed = 1)\n    model_base.fit(X_train, y_train, eval_metric='rmse')\n\n    preds = model_base.predict(X_test.values)\n\n    preds_df = pd.DataFrame(preds,columns=['item_cnt_month'])\n    preds_df['ID'] = preds_df.index\n    preds_df = preds_df.set_index('ID')\n    preds_df.to_csv('submision1.csv')\n    display(preds_df)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:08:41.079915Z","iopub.execute_input":"2021-10-15T03:08:41.080227Z","iopub.status.idle":"2021-10-15T03:08:41.089546Z","shell.execute_reply.started":"2021-10-15T03:08:41.080199Z","shell.execute_reply":"2021-10-15T03:08:41.088588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dividir mejor el train para hacer stacking","metadata":{}},{"cell_type":"code","source":"if True:\n    msk = train_test['date_block_num']<33\n    X_train = train_test[msk]\n    y_train = X_train['item_cnt_month']\n    \n    X_train = X_train.drop(columns=['item_cnt_month','item_price'])\n    \n    \n    msk = train_test['date_block_num'] == 33\n    X_val = train_test[msk]\n    y_val = X_val['item_cnt_month'].astype('int8')\n    \n    X_val = X_val.drop(columns=['item_cnt_month','item_price'])\n    \n    msk = train_test['date_block_num'] == 34\n    X_test = train_test[msk]\n    X_test = X_test.drop(columns=['item_cnt_month','item_price'])\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:08:41.091337Z","iopub.execute_input":"2021-10-15T03:08:41.091663Z","iopub.status.idle":"2021-10-15T03:08:42.55162Z","shell.execute_reply.started":"2021-10-15T03:08:41.091623Z","shell.execute_reply":"2021-10-15T03:08:42.55088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_test\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:08:42.553259Z","iopub.execute_input":"2021-10-15T03:08:42.553572Z","iopub.status.idle":"2021-10-15T03:08:42.75721Z","shell.execute_reply.started":"2021-10-15T03:08:42.553533Z","shell.execute_reply":"2021-10-15T03:08:42.756126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"if partes_activas[3]:\n    XGBR_model = xgb.XGBRegressor(max_depth = 10, min_child_weight=0.5, subsample = 1, eta = 0.3, n_estimators = 1000, seed = 1)\n    XGBR_model.fit(X_train, y_train, eval_metric='rmse')\n\n    XGBR_test_preds = XGBR_model.predict(X_test.values)\n    XGBR_val_preds = XGBR_model.predict(X_val.values)\n\n    XGBpreds_df = pd.DataFrame(XGBR_test_preds,columns=['item_cnt_month'])\n    XGBpreds_df['ID'] = XGBpreds_df.index\n    XGBpreds_df = XGBpreds_df.set_index('ID')\n    XGBpreds_df['item_cnt_month'] = XGBpreds_df['item_cnt_month'].clip(0,20)\n    XGBpreds_df.to_csv('submisionXBG.csv')\n    \n    XGB_val_preds_df = pd.DataFrame(XGBR_val_preds,columns=['item_cnt_month'])\n    XGB_val_preds_df['ID'] = XGB_val_preds_df.index\n    XGB_val_preds_df = XGB_val_preds_df.set_index('ID')\n    XGB_val_preds_df['item_cnt_month'] =  XGB_val_preds_df['item_cnt_month'].clip(0,20) \n    XGB_val_preds_df.to_csv('XBG_val_preds.csv')\n    \n    \n    display(XGBpreds_df)\n    \n    del XGBR_model\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:08:42.758584Z","iopub.execute_input":"2021-10-15T03:08:42.758848Z","iopub.status.idle":"2021-10-15T03:08:42.768634Z","shell.execute_reply.started":"2021-10-15T03:08:42.758818Z","shell.execute_reply":"2021-10-15T03:08:42.767941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"en los nucleos de kaggle se tardó 3h en entrenar con 4 meses de features y mas de 6h con un año y elo vale la pena usar los 12 meses","metadata":{}},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"def build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features):\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_val = lgb.Dataset(X_val, y_val)\n    model = lgb.train(params=params, train_set=lgb_train, valid_sets=(lgb_train, lgb_val), verbose_eval=50,\n                     categorical_feature=cat_features)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:08:42.769596Z","iopub.execute_input":"2021-10-15T03:08:42.769846Z","iopub.status.idle":"2021-10-15T03:08:42.787148Z","shell.execute_reply.started":"2021-10-15T03:08:42.769819Z","shell.execute_reply":"2021-10-15T03:08:42.785992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if partes_activas[4]:\n    #skip this cell if directly loading saved model \n    params = {\n        'objective': 'rmse',\n        'metric': 'rmse',\n        'num_leaves': 1023,\n        'min_data_in_leaf':10,\n        'feature_fraction':0.7,\n        'learning_rate': 0.01,\n        'num_rounds': 1000,\n        'early_stopping_rounds': 30,\n        'seed': 1\n    }\n    #designating the categorical features which should be focused on\n    cat_features = ['date_block_num','shop_id','item_id']\n\n    lgb_model = build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features)\n    \n    LGBM_test_preds = lgb_model.predict(X_test).clip(0,20)\n    LGBM_val_preds = lgb_model.predict(X_val).clip(0,20)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:08:42.788406Z","iopub.execute_input":"2021-10-15T03:08:42.788731Z","iopub.status.idle":"2021-10-15T03:08:42.800135Z","shell.execute_reply.started":"2021-10-15T03:08:42.788689Z","shell.execute_reply":"2021-10-15T03:08:42.799278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if partes_activas[4]:\n    LGBpreds_df = pd.DataFrame(LGBM_test_preds,columns=['item_cnt_month'])\n    LGBpreds_df['ID'] = LGBpreds_df.index\n    LGBpreds_df = LGBpreds_df.set_index('ID')\n    LGBpreds_df['item_cnt_month'] = LGBpreds_df['item_cnt_month'].clip(0,20)\n    LGBpreds_df.to_csv('submisionLGBM.csv')\n    \n    LGB_val_preds_df = pd.DataFrame(LGBM_val_preds,columns=['item_cnt_month'])\n    LGB_val_preds_df['ID'] = LGB_val_preds_df.index\n    LGB_val_preds_df = LGB_val_preds_df.set_index('ID')\n    LGB_val_preds_df['item_cnt_month'] =  LGB_val_preds_df['item_cnt_month'].clip(0,20) \n    LGB_val_preds_df.to_csv('LGBM_val_preds.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:08:42.801304Z","iopub.execute_input":"2021-10-15T03:08:42.801598Z","iopub.status.idle":"2021-10-15T03:08:42.81635Z","shell.execute_reply.started":"2021-10-15T03:08:42.801569Z","shell.execute_reply":"2021-10-15T03:08:42.815575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM \nNo pude hacer correr un entrenamiento en el núcleo de kaggle","metadata":{}},{"cell_type":"code","source":"\nif partes_activas[5]:\n    svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n    svr_lin = SVR(kernel='linear', C=5, gamma='auto')\n    svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,\n                   coef0=1)\n\n    allSVM=[svr_lin]\n    \n    \n    \n    for i,VM in enumerate(allSVM):\n\n        VM.fit(X_train, y_train)\n        nombre='SVM_val_'+str(i)+'.csv'\n        SVM_val_preds=VM.predict(X_val)\n\n        SVM_val_preds_df = pd.DataFrame(SVM_val_preds,columns=['item_cnt_month'])\n        SVM_val_preds_df['ID'] = SVM_val_preds_df.index\n        SVM_val_preds_df = SVM_val_preds_df.set_index('ID')\n        SVM_val_preds_df['item_cnt_month'] =  SVM_val_preds_df['item_cnt_month'].clip(0,20) \n        SVM_val_preds_df.to_csv(nombre)\n\n\n        nombre='SVM_test_'+str(i)+'.csv'\n        SVM_test_preds=VM.predict(X_test)\n\n        SVM_test_preds_df = pd.DataFrame(SVM_test_preds,columns=['item_cnt_month'])\n        SVM_test_preds_df['ID'] = SVM_test_preds_df.index\n        SVM_test_preds_df = SVM_test_preds_df.set_index('ID')\n        SVM_test_preds_df['item_cnt_month'] =  SVM_test_preds_df['item_cnt_month'].clip(0,20) \n        SVM_test_preds_df.to_csv(nombre)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-15T03:08:42.817303Z","iopub.execute_input":"2021-10-15T03:08:42.817599Z","iopub.status.idle":"2021-10-15T03:08:42.830671Z","shell.execute_reply.started":"2021-10-15T03:08:42.817501Z","shell.execute_reply":"2021-10-15T03:08:42.829626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ahora intento algo de Deeplearning","metadata":{}},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    'Characterizes a dataset for PyTorch'\n    def __init__(self, list_IDs, labels):\n        'Initialization'\n        self.labels = labels\n        self.list_IDs = list_IDs\n        \n    def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.list_IDs)\n\n    def __getitem__(self, index):\n        'Generates one sample of data'\n        \n        X = self.list_IDs[index]\n        \n        y = self.labels[index]\n        \n\n        return X, y\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-14T03:18:04.555985Z","iopub.execute_input":"2021-10-14T03:18:04.556459Z","iopub.status.idle":"2021-10-14T03:18:04.566091Z","shell.execute_reply.started":"2021-10-14T03:18:04.556422Z","shell.execute_reply":"2021-10-14T03:18:04.565324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_trainT = torch.tensor(y_train.values)\ndel y_train\nX_trainT = torch.tensor(X_train.values)\ndel X_train\ny_valT = torch.tensor(y_val.values)\nX_valT = torch.tensor(X_val.values)\nX_testT = torch.tensor(X_test.values)\nnumero_features = X_trainT.size()[1]\n","metadata":{"execution":{"iopub.status.busy":"2021-10-14T03:18:04.567409Z","iopub.execute_input":"2021-10-14T03:18:04.567645Z","iopub.status.idle":"2021-10-14T03:18:06.15892Z","shell.execute_reply.started":"2021-10-14T03:18:04.567606Z","shell.execute_reply":"2021-10-14T03:18:06.158126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-14T03:18:06.160412Z","iopub.execute_input":"2021-10-14T03:18:06.160695Z","iopub.status.idle":"2021-10-14T03:18:06.287219Z","shell.execute_reply.started":"2021-10-14T03:18:06.160653Z","shell.execute_reply":"2021-10-14T03:18:06.28646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_DL(DSE,labels):\n    DS = Dataset(DSE, labels)\n    generator = DataLoader(DS,batch_size=102400, shuffle=False)\n    return DS , generator \n    \n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-14T03:18:06.292377Z","iopub.execute_input":"2021-10-14T03:18:06.292691Z","iopub.status.idle":"2021-10-14T03:18:06.298121Z","shell.execute_reply.started":"2021-10-14T03:18:06.292651Z","shell.execute_reply":"2021-10-14T03:18:06.297018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_trainT, train_dl = make_DL(X_trainT,y_trainT)\nX_valT, val_dl = make_DL(X_valT, y_valT)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-14T03:18:06.29967Z","iopub.execute_input":"2021-10-14T03:18:06.299949Z","iopub.status.idle":"2021-10-14T03:18:06.30917Z","shell.execute_reply.started":"2021-10-14T03:18:06.299912Z","shell.execute_reply":"2021-10-14T03:18:06.308537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\nFirst iteration of data set: ', next(iter(train_dl)), '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-10-14T03:18:06.310639Z","iopub.execute_input":"2021-10-14T03:18:06.310936Z","iopub.status.idle":"2021-10-14T03:18:07.640202Z","shell.execute_reply.started":"2021-10-14T03:18:06.310897Z","shell.execute_reply":"2021-10-14T03:18:07.639462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-14T03:18:07.641392Z","iopub.execute_input":"2021-10-14T03:18:07.642094Z","iopub.status.idle":"2021-10-14T03:18:07.698917Z","shell.execute_reply.started":"2021-10-14T03:18:07.642054Z","shell.execute_reply":"2021-10-14T03:18:07.697976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#nn package to create our linear model\n# each Linear module has a weight and bias\n\ntorch.manual_seed(11)\nlearning_rate = 1e-3\nlambda_l2 = 1e-5\n\nF = numero_features\nH1 = 512\nH2 = 1024\nH3 = 1024\nH4 = 64\nH5 = 16\nS = 1\n\n\n\nA1MLP = nn.Sequential(\n    nn.Linear(F, H2),\n    nn.ReLU(),\n    nn.Linear(H2, H4),\n    nn.ReLU(),\n    nn.Linear(H4, S),\n    \n)\n\nA2MLP = nn.Sequential(\n    nn.Linear(F, H1),\n    nn.ReLU(),\n    nn.Linear(H1, H2),\n    nn.ReLU(),\n    nn.Linear(H2, H3),\n    nn.ReLU(),\n    nn.Dropout(),\n    nn.Linear(H3, H4),\n    nn.ReLU(),\n    nn.Linear(H4, H5),\n    nn.ReLU(),\n    nn.Linear(H5, S),\n    \n)\n\nMLP_arch =[A1MLP,A2MLP]\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-14T03:18:07.700015Z","iopub.execute_input":"2021-10-14T03:18:07.700308Z","iopub.status.idle":"2021-10-14T03:18:07.728154Z","shell.execute_reply.started":"2021-10-14T03:18:07.700252Z","shell.execute_reply":"2021-10-14T03:18:07.727547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if partes_activas[6]:\n    \n    for i,MLP in enumerate(MLP_arch):\n    \n        MLP.to(device)\n\n\n        criterion = torch.nn.MSELoss()\n\n        # we use the optim package to apply\n        # ADAM for our parameter updates\n        optimizer = torch.optim.Adam(MLP.parameters(), lr=learning_rate, weight_decay=lambda_l2) \n\n        epochs = 3+i\n\n        for epoch in range(epochs):\n        # Training\n            MLP.train()\n            epoch_loss=0\n            for batch in tqdm(train_dl):\n                # Feed forward to get the logits\n                Xmin,ymin = batch\n                Xmin = Xmin.float()\n                ymin=ymin.float()\n                y_pred = torch.squeeze(MLP(Xmin.to(device)))\n\n                # Compute the loss \n                loss = criterion(y_pred.to(device), ymin.to(device))\n                epoch_loss = loss.item() + epoch_loss\n                #print(epoch_loss)\n                #acc = (y == predicted).sum().float() / len(y)\n\n                #display.clear_output(wait=True)\n\n                # zero the gradients before running\n                # the backward pass.\n                optimizer.zero_grad()\n\n                # Backward pass to compute the gradient\n                # of loss w.r.t our learnable params. \n                loss.backward()\n\n                # Update params\n                optimizer.step()\n            epoch_loss = epoch_loss/len(train_dl)\n\n\n            #Validation Step\n            MLP.eval()\n            val_loss=0\n            y_pred_list=[]\n            for batch in tqdm(val_dl):\n                Xmin,ymin = batch\n                Xmin = Xmin.float()\n                ymin=ymin.float()\n\n                y_pred = torch.squeeze(MLP(Xmin.to(device)))\n                y_pred_list = y_pred_list + y_pred.tolist()\n\n                lossV = criterion(y_pred.to(device), ymin.to(device))\n                val_loss = lossV.item() + val_loss\n            val_loss = val_loss/len(val_dl)\n\n            val_score = mean_squared_error(y_val,np.array(y_pred_list),squared=False)\n\n            print(\"[EPOCH]: %i, [LOSS_TRAIN]: %.6f, [LOSS_VAL]: %.6f, [VAL_SCORE]: %.6f\" % (epoch, epoch_loss, val_loss, val_score))\n\n        with torch.no_grad():\n            MLPpreds = MLP(X_testT.float().to(device))\n\n        MLPpreds_df = pd.DataFrame(MLPpreds.cpu().numpy(),columns=['item_cnt_month'])\n        MLPpreds_df['ID'] = MLPpreds_df.index\n        MLPpreds_df = MLPpreds_df.set_index('ID')\n        MLPpreds_df['item_cnt_month'] = MLPpreds_df['item_cnt_month'].clip(0,20)\n        MLPpreds_df.to_csv('submisionMLP'+str(i)+'.csv')\n\n\n        MLP_val_preds_df = pd.DataFrame(y_pred_list,columns=['item_cnt_month'])\n        MLP_val_preds_df['ID'] = MLP_val_preds_df.index\n        MLP_val_preds_df = MLP_val_preds_df.set_index('ID')\n        MLP_val_preds_df['item_cnt_month']=MLP_val_preds_df['item_cnt_month'].clip(0,20)\n        MLP_val_preds_df.to_csv('MLP_val_preds'+str(i)+'.csv')\n\n        display(MLPpreds_df)\n\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2021-10-14T03:32:37.943635Z","iopub.execute_input":"2021-10-14T03:32:37.943898Z","iopub.status.idle":"2021-10-14T03:34:55.193835Z","shell.execute_reply.started":"2021-10-14T03:32:37.943869Z","shell.execute_reply":"2021-10-14T03:34:55.192897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking con los modelos","metadata":{}},{"cell_type":"code","source":"if partes_activas[7]:\n    \n    \n    MLP_val_preds_df1  = pd.read_csv('../input/sales-cv-for-s/MLP_val_preds1.csv')\n    MLP_val_preds_df1['item_cnt_month']=MLP_val_preds_df1['item_cnt_month'].clip(0,20)\n    \n    MLPpreds_df1 =  pd.read_csv('../input/sales-cv-for-s/submisionMLP1.csv')\n    \n    \n    \n    MLP_val_preds_df2  = pd.read_csv('../input/sales-cv-for-s/MLP_val_preds0.csv')\n    MLP_val_preds_df2['item_cnt_month']=MLP_val_preds_df2['item_cnt_month'].clip(0,20)\n    \n    MLPpreds_df2 =  pd.read_csv('../input/sales-cv-for-s/submisionMLP0.csv')\n    \n    XGB_val_preds_df = pd.read_csv('../input/sales-cv-for-s/XBG_val_preds.csv')\n    XGB_val_preds_df['item_cnt_month'] =  XGB_val_preds_df['item_cnt_month'].clip(0,20) \n    \n    XGBpreds_df = pd.read_csv('../input/sales-cv-for-s/submisionXBG.csv')\n    \n    LGB_val_preds_df = pd.read_csv('../input/sales-cv-for-s/LGBM_val_preds.csv')\n    LGB_val_preds_df['item_cnt_month'] =  LGB_val_preds_df['item_cnt_month'].clip(0,20) \n    \n    LGBpreds_df = pd.read_csv('../input/sales-cv-for-s/submisionLGBM.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-14T03:24:59.782404Z","iopub.execute_input":"2021-10-14T03:24:59.7827Z","iopub.status.idle":"2021-10-14T03:24:59.79105Z","shell.execute_reply.started":"2021-10-14T03:24:59.78266Z","shell.execute_reply":"2021-10-14T03:24:59.790075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val.reset_index().head(30)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T03:24:59.79222Z","iopub.execute_input":"2021-10-14T03:24:59.792682Z","iopub.status.idle":"2021-10-14T03:24:59.814612Z","shell.execute_reply.started":"2021-10-14T03:24:59.792639Z","shell.execute_reply":"2021-10-14T03:24:59.813789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if partes_activas[8]:\n\n    X_train_level2= pd.concat([MLP_val_preds_df1['item_cnt_month'],XGB_val_preds_df['item_cnt_month'],LGB_val_preds_df['item_cnt_month'], MLP_val_preds_df2['item_cnt_month']],axis=1)\n    X_test_level2 = pd.concat([MLPpreds_df1['item_cnt_month'],XGBpreds_df['item_cnt_month'],LGBpreds_df['item_cnt_month'],MLPpreds_df2['item_cnt_month']],axis=1)\n\n    from sklearn.linear_model import LinearRegression as lr\n    from sklearn.ensemble import RandomForestRegressor\n\n\n    #meta_model = SVR(kernel='linear', C=20, gamma='auto',verbose=True)\n    meta_model = lr()\n    meta_model.fit(X_train_level2, y_val)\n\n    predsMeta=meta_model.predict(X_test_level2)\n\n\n\n    Meta_preds_df = pd.DataFrame(predsMeta,columns=['item_cnt_month'])\n    Meta_preds_df['ID'] = Meta_preds_df.index\n    Meta_preds_df = Meta_preds_df.set_index('ID')\n    Meta_preds_df['item_cnt_month'] = Meta_preds_df['item_cnt_month'].clip(0,20)\n    Meta_preds_df.to_csv('submisionMeta.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-14T03:24:59.816013Z","iopub.execute_input":"2021-10-14T03:24:59.816293Z","iopub.status.idle":"2021-10-14T03:24:59.994761Z","shell.execute_reply.started":"2021-10-14T03:24:59.816259Z","shell.execute_reply":"2021-10-14T03:24:59.993621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Meta_preds_df.head(30)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T03:24:59.996339Z","iopub.status.idle":"2021-10-14T03:24:59.99679Z","shell.execute_reply.started":"2021-10-14T03:24:59.996554Z","shell.execute_reply":"2021-10-14T03:24:59.996578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}